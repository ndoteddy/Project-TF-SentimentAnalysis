"""MyMachineLearningModel.ipynb

Automatically generated by Colab.
  Author : Hernando Ivan Teddy
Topic : Sentiment Analysis
LSTM (Long Short-Term Memory) layer or GRU (Gated Recurrent Unit) for sequence processing.
Original file is located at
    https://colab.research.google.com/drive/1TljWOOrx-E5dlwjs65z2VpWuEH4YqIgB
"""

# -*- coding: utf-8 -*-
!pip install tensorflow
!pip install numpy
!pip install pandas
!pip install scikit-learn

import pandas as pd
import random

# Dummy sentences with three sentiment categories
positive_sentences = [
    "I love this!", "This is amazing!", "Absolutely fantastic!", "I feel great!", "I'm so happy with this!",
    "Wonderful experience!", "Totally recommend this.", "I am very impressed!", "Best purchase ever!", "I'm very satisfied!",
    "I feel awesome!", "This is incredible!", "So good!", "This product is perfect!", "I’m so pleased with it.",
    "Absolutely love it!", "This exceeded my expectations!", "Best decision ever!", "Really happy with this!",
    "Perfect, couldn’t be better!", "Exceeded my expectations!", "Couldn’t be more satisfied!", "Highly recommend!",
    "I’m in love with this product!", "It’s the best!", "Couldn’t ask for more!", "This is exactly what I needed!",
    "Totally worth the price!", "This is exactly what I was looking for!", "I feel great using this."
]

neutral_sentences = [
    "It's okay.", "It’s just average.", "Nothing special.", "It’s fine, I guess.", "It’s neither good nor bad.",
    "I feel indifferent about this.", "It works, but I’m not excited.", "It’s decent, but not great.", "It’s alright.",
    "Okay, I suppose.", "It’s alright, nothing more.", "Nothing remarkable.", "Just as expected.", "It does the job.",
    "It’s a normal product.", "It’s not bad, but not great either.", "It’s okay, could be better.", "It’s just fine.",
    "Nothing stands out.", "It's passable.", "It’s neither here nor there.", "I can take it or leave it.",
    "Could be better, could be worse.", "It’s neither amazing nor awful.", "Meh, it’s okay.", "Not great, not bad.",
    "I can’t complain, but I’m not thrilled.", "It’s just like any other product.", "It works, but I’m not wowed."
]

negative_sentences = [
    "I hate this!", "This is horrible!", "Worst experience ever.", "I’m so disappointed.", "I regret this purchase.",
    "It’s terrible.", "Not worth the money.", "I will never buy this again.", "The worst decision I’ve ever made.",
    "Very unsatisfactory.", "I wouldn’t recommend this.", "Horrible quality.", "Completely useless.", "Waste of money.",
    "Extremely bad!", "This is a rip-off!", "I’m really unhappy with this.", "The worst product I’ve bought.",
    "Never again.", "This is awful.", "It didn’t work at all.", "I can’t believe I bought this.", "What a disappointment.",
    "Very poor quality.", "Terrible purchase.", "This is frustrating!", "I am so dissatisfied.", "I’m furious!",
    "I will never recommend this.", "Worst purchase ever!", "I hate everything about this.", "Not as advertised.",
    "This ruined my day.", "I can’t believe this failed me.", "Very disappointed with this product.", "I am so upset with this.",
    "This product is a joke.", "Complete waste of time.", "Never buying from here again.", "Extremely let down.",
    "Worst experience ever.", "Not even close to what I expected.", "This made me regret buying it.", "Never buying this again."
]

# Create the dataset with exactly 100 records (33 positive, 33 neutral, 34 negative)
data = []

for _ in range(34):
    sentence = random.choice(negative_sentences)
    data.append((sentence, 0))  # Negative

for _ in range(33):
    sentence = random.choice(neutral_sentences)
    data.append((sentence, 1))  # Neutral

for _ in range(33):
    sentence = random.choice(positive_sentences)
    data.append((sentence, 2))  # Positive

# Convert to DataFrame
df = pd.DataFrame(data, columns=['sentence', 'sentiment'])

# Show the distribution of sentiment labels in the dataset
distribution = df['sentiment'].value_counts()

# Print the distribution
print("Sentiment Distribution:")
print(distribution)

# Optionally, plot the distribution using a bar chart
import matplotlib.pyplot as plt

# Plot the distribution
distribution.plot(kind='bar', color=['red', 'gray', 'green'])
plt.title("Sentiment Distribution")
plt.xlabel("Sentiment Class")
plt.ylabel("Number of Samples")
plt.xticks(ticks=[0, 1, 2], labels=['Negative (0)', 'Neutral (1)', 'Positive (2)'], rotation=0)
plt.show()

"""Step 3: Preprocessing the Data
Text Tokenization: Convert text into a format suitable for machine learning (e.g., tokens or embeddings).

Splitting Data: Split the data into training and testing datasets.

"""

# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.utils import to_categorical

# 1. Preprocess the data
# Load the dataset (from previous code)
# df = ...

# Split data into train and test sets
X = df['sentence']
y = df['sentiment']
y = to_categorical(y, num_classes=3)  # One-hot encoding for multi-class classification

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Tokenize the sentences
tokenizer = Tokenizer(num_words=5000)  # Limit to the top 5000 words
tokenizer.fit_on_texts(X_train)

X_train_seq = tokenizer.texts_to_sequences(X_train)
X_test_seq = tokenizer.texts_to_sequences(X_test)

# Pad sequences to have uniform length
max_length = max([len(x) for x in X_train_seq])  # Length of the longest sentence in training set
X_train_pad = pad_sequences(X_train_seq, maxlen=max_length, padding='post')
X_test_pad = pad_sequences(X_test_seq, maxlen=max_length, padding='post')

# 2. Build the model
model = Sequential()
model.add(Embedding(input_dim=5000, output_dim=128, input_length=max_length))  # Embedding layer
model.add(LSTM(128, return_sequences=False))  # LSTM layer
model.add(Dropout(0.2))  # Dropout to reduce overfitting
model.add(Dense(3, activation='softmax'))  # Output layer with 3 classes

# 3. Compile the model
model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])

# 4. Train the model
history = model.fit(X_train_pad, y_train, epochs=5, batch_size=32, validation_data=(X_test_pad, y_test))

# 5. Evaluate the model
accuracy = model.evaluate(X_test_pad, y_test)
print(f"Test accuracy: {accuracy[1]*100:.2f}%")

# Optionally, plot training and validation accuracy and loss
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Accuracy')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Loss')
plt.legend()

plt.show()

"""Step 4: Build the Sentiment Analysis ModelUse a simple neural network for sentiment analysis (e.g., LSTM or Dense layers).

python
Copy
Edit


"""

# Save the model
model.save('sentiment_analysis_model.h5')

"""View and download the file

"""

!ls -l
from google.colab import files
files.download('sentiment_analysis_model.h5')

"""Upload to cloud storage

"""

# from google.colab import auth
# from google.cloud import storage

# auth.authenticate_user()

# # Set your GCS bucket name
# bucket_name = 'mymachine'  # <- Change this
# destination_blob_name = 'models/sentiment_analysis_model.h5'

# # Upload the file
# client = storage.Client()
# bucket = client.bucket(bucket_name)
# blob = bucket.blob(destination_blob_name)
# blob.upload_from_filename('sentiment_analysis_model.h5')

# print(f'Model uploaded to gs://{bucket_name}/{destination_blob_name}')